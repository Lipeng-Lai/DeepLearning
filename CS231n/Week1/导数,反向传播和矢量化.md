## 1.导数

### 1.1标量情况

在标量情况下导数的概念给定一个函数$f$，在$x$点定义为
$$
f'(x) = \lim_{h\rightarrow 0} \frac{f(x+h) - f(x)}{h}
$$
描述了$x$发生微小变形时，函数$f$发生的变化
$$
f(x + \epsilon) \approx f(x) + \epsilon f'(x)
$$

$$
x \rightarrow x + \Delta x  \\
y \rightarrow \approx y + \frac{\partial y}{\partial x} \Delta x
$$

在标量情况下，假设有$y = f(x), z = g(y)$
$$
x \rightarrow^f \,y \rightarrow^g \, z
$$
根据求导的链式法则有
$$
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}
$$

### 1.2梯度：矢量入,标量出

以一个向量作为输入并产生一个标量，$f$在点$x \in R^N$处的导数被称为梯度
$$
\nabla_x f(x) = \lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{||h||}
$$
如果设$y = f(x)$就得到了以下这个关系
$$
x \rightarrow x + \Delta x \\
y \rightarrow \approx y + \frac{\partial y}{\partial x} \Delta x
$$
与标量下求导不同的是，$x, \Delta x, \frac{\partial y}{\partial x}$是向量，然而$y$是标量，特别的，当$\frac{\partial y}{\partial x}$乘以$\Delta x$时，是点乘$\cdot$，它们的结果是标量



设$\Delta x$是第$i$个基向量，使得$\epsilon$的第$i$个坐标为1，$\epsilon$的其他所有坐标为0

那么$\frac{\partial y}{\partial x} \cdot \Delta x$的点积结果是$\frac{\partial y}{\partial x}$的第$i$个坐标，因此$\frac{\partial y}{\partial x}$告诉了我们当$x$

沿着第$i$个坐标移动时$y$的大概变化量

意味着，可以把梯度$\frac{\partial y}{\partial x}$看作一个偏导数的向量
$$
\frac{\partial y}{\partial x} = (\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2},...,\frac{\partial y}{\partial x_N})
$$
其中$x_i$是向量$x$的第$i$个坐标，$x_i, \frac{\partial y}{\partial x_i}$均为标量



### 1.3雅可比矩阵：矢量入,矢量出

一个矢量作为输入，产生一个矢量作为输出，$f$在$x$处的到导数，也叫雅可比矩阵，是$M \times N$的偏导数矩阵，令$y = f(x)$
$$
\frac{\partial y}{\partial x} = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1}&...&\frac{\partial y_1}{\partial x_N} \\
...&...&... \\
\frac{\partial y_M}{\partial x_1} &...&\frac{\partial y_M}{\partial x_N}
\end{bmatrix}
$$
同样地雅可比矩阵也有输入和输出的关系
$$
x \rightarrow x + \Delta x \\
y \rightarrow \approx y + \frac{\partial y}{\partial x} \Delta x
$$
其中，$\frac{\partial y}{\partial x}$是一个$M \times N$矩阵，$\Delta x$是一个$N$维向量，二者相乘得到一个$M$维向量



### 1.4广义雅可比矩阵：张量输入,张量输出

张量是$D$维的数字网格
$$
f:R^{N_1 \times ...\times N_{D_x}} \rightarrow R^{M_1 \times ...\times M_{D_y}}
$$
如果$y = f(x)$，那么$\frac{\partial y}{\partial x}$即为广义雅可比矩阵
$$
(M_1 \times ... \times M_{D_y}) \times (N_1 \times .. \times N_{D_x})
$$
将$\frac{\partial y}{\partial x}$的维数分成两组，第一组与$y$的维数相匹配，第二组与$x$的维数相匹配，通过这种分组，可将广义雅可比矩阵视为矩阵的泛化，其中"每一行与$y$具有相同的形状"，"每一列与$x$具有相同的形状"



如果我们让$i \in Z^{D_y}, j \in Z^{D_x}$为向量的整数下标，那么可以写为
$$
(\frac{\partial y}{\partial x})_{i,j} = \frac{\partial y_i}{\partial x_j}
$$
广义雅可比矩阵给出了与之前相同的输入和输出关系:
$$
x \rightarrow x + \Delta x \\
y \rightarrow \approx y + \frac{\partial y}{\partial x} \Delta x
$$
其中， $\Delta x$是形状为$N_1 \times ... \times N_{D_x}$的张量，$\frac{\partial y}{\partial x}$是形状为$(M_1 \times ... \times M_{D_y}) \times (N_1 \times ... \times N_{D_x})$的广义矩阵，其乘积结果为

形状为$M_1 \times ... \times M_{D_y}$的张量



## 2.张量的反向传播

在反向传播的过程中，根据链式法则有
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial x}, \frac{\partial L}{\partial w} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial w}
$$
由于数据量过大，因此显式地存储和操作雅可比矩阵是没有希望的

然而，对于大多数常见的神经网络，可以推导出$\frac{\partial y}{\partial x}, \frac{\partial L}{\partial y}$乘积的表达式，而无需显式地形成$\frac{\partial y}{\partial x}$的雅可比矩阵



设线性层$f(x,w) = xw, N=1,D=2,M=3$
$$
y = (y_{1,1}, y_{1,2}, y_{1,3}) = xw \\
= (x_{1,1}, x_{1,2}) \begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} \\
w_{2,1} & w_{2,2} & w_{2,3}
\end{bmatrix}
$$
可得一个形状为$N \times M$的矩阵
$$
\frac{\partial L}{\partial y} = (dy_{1,1}, dy_{1,2}, dy_{1,3})
$$
分别考虑元素有
$$
\frac{\partial L}{\partial x_{1,1}} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial x_{1,1}}
$$
其中$\frac{\partial L}{\partial y} \frac{\partial y}{\partial x_{1,1}}$的形状分别为：$(1 \times N \times M), (N \times M \times 1)$

它们的乘积$\frac{\partial L}{\partial x_{1,1}}$的形状为$(1 \times 1)$
$$
\frac{\partial y}{\partial x_{1,1}} = (\frac{\partial y_{1,1}}{\partial x_{1,1}},\frac{\partial y_{1,2}}{\partial x_{1,2}},\frac{\partial y_{1,3}}{\partial x_{1,3}}) \\
= (w_{1,1}, w_{1,2}, w_{1,3})
$$
结合有
$$
\frac{\partial L}{\partial x_{1,1}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x_{1,1}} = dy_{1,1} w_{1,1} + dy_{1,2}w_{1,2} + dy_{1,3}w_{1,3}
$$
整理出一般规律有
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}w^T
$$

$$
\frac{\partial L}{\partial w} = x^T \frac{\partial L}{\partial y}
$$

$$
\frac{\partial L}{\partial b} = \sum_{row=1} \frac{\partial L}{\partial y}
$$

