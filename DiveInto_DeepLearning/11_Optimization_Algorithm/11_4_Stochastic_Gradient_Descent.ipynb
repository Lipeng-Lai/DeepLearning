{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度更新\n",
    "\n",
    "目标函数\n",
    "$$\n",
    "f(x) = \\frac{1}{n} \\sum_{i=1}^n f_i (x)\n",
    "$$\n",
    "\n",
    "其梯度计算为\n",
    "$$\n",
    "\\nabla f(x) = \\frac{1}{n}\\sum_{i=1}^n \\nabla f_i(x)\n",
    "$$\n",
    "\n",
    "如果使用梯度下降法，则每个自变量迭代的计算代价为$O(n)$，它随$n$线性增长，\n",
    "当训练数据集较大时，每次迭代的梯度下降计算代价将较高\n",
    "\n",
    "\n",
    "随机梯度下降(SGD)，可降低每次迭代时的计算代价，在随机梯度下降的每次迭代中，对数据\n",
    "样本随机均匀采样一个索引$i$，其中$i \\in 1,...,n$，并计算梯度$\\nabla f_i(x)$更新$x$\n",
    "\n",
    "$$\n",
    "x \\leftarrow x - \\xi \\nabla f_i(x)\n",
    "$$\n",
    "\n",
    "每次迭代的计算代价从梯度下降的$O(n)$下降到常数$O(1)$，此时随机梯度$\\nabla f_i(x)$\n",
    "是对完整梯度$\\nabla f(x)$的无偏估计\n",
    "\n",
    "$$\n",
    "E_i \\nabla f_i(x) = \\frac{1}{n}\\sum_{i=1}^n \\nabla f_i(x) = \\nabla f(x)\n",
    "$$\n",
    "\n",
    "对于平均而言，随机梯度是对梯度的良好估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_2d() got an unexpected keyword argument 'f_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m eta \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m     19\u001b[0m lr \u001b[39m=\u001b[39m constant_lr  \u001b[39m# 常数学习速度\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m d2l\u001b[39m.\u001b[39mshow_trace_2d(f, d2l\u001b[39m.\u001b[39;49mtrain_2d(sgd, steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, f_grad\u001b[39m=\u001b[39;49mf_grad))\n",
      "\u001b[1;31mTypeError\u001b[0m: train_2d() got an unexpected keyword argument 'f_grad'"
     ]
    }
   ],
   "source": [
    "def f(x1, x2):  # 目标函数\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_grad(x1, x2):  # 目标函数的梯度\n",
    "    return 2 * x1, 4 * x2\n",
    "\n",
    "def sgd(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    # 模拟有噪声的梯度\n",
    "    g1 += torch.normal(0.0, 1, (1,))\n",
    "    g2 += torch.normal(0.0, 1, (1,))\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n",
    "\n",
    "def constant_lr():\n",
    "    return 1\n",
    "\n",
    "eta = 0.1\n",
    "lr = constant_lr  # 常数学习速度\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态学习率\n",
    "\n",
    "如果太快，则过早停止优化；如果太慢，则在优化上浪费太多时间\n",
    "\n",
    "指数衰减\n",
    "\n",
    "$$\n",
    "\\xi(t) = \\xi_0 e^{-\\lambda t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_2d() got an unexpected keyword argument 'f_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m t \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      8\u001b[0m lr \u001b[39m=\u001b[39m exponential_lr\n\u001b[1;32m----> 9\u001b[0m d2l\u001b[39m.\u001b[39mshow_trace_2d(f, d2l\u001b[39m.\u001b[39;49mtrain_2d(sgd, steps\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, f_grad\u001b[39m=\u001b[39;49mf_grad))\n",
      "\u001b[1;31mTypeError\u001b[0m: train_2d() got an unexpected keyword argument 'f_grad'"
     ]
    }
   ],
   "source": [
    "def exponential_lr():\n",
    "    # 在函数外部定义，而在内部更新的全局变量\n",
    "    global t\n",
    "    t += 1\n",
    "    return math.exp(-0.1 * t)\n",
    "\n",
    "t = 1\n",
    "lr = exponential_lr\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_2d() got an unexpected keyword argument 'f_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m t \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      8\u001b[0m lr \u001b[39m=\u001b[39m polynomial_lr\n\u001b[1;32m----> 9\u001b[0m d2l\u001b[39m.\u001b[39mshow_trace_2d(f, d2l\u001b[39m.\u001b[39;49mtrain_2d(sgd, steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, f_grad\u001b[39m=\u001b[39;49mf_grad))\n",
      "\u001b[1;31mTypeError\u001b[0m: train_2d() got an unexpected keyword argument 'f_grad'"
     ]
    }
   ],
   "source": [
    "def polynomial_lr():\n",
    "    # 在函数外部定义，而在内部更新的全局变量\n",
    "    global t\n",
    "    t += 1\n",
    "    return (1 + 0.1 * t) ** (-0.5)\n",
    "\n",
    "t = 1\n",
    "lr = polynomial_lr\n",
    "d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
